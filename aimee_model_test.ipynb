{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f6e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade \"protobuf<=3.20.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c67b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 04:59:01.973958: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-17 04:59:02.027760: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-17 04:59:02.029201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-17 04:59:03.192749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'np_utils' from 'keras.utils' (/home/codespace/.python/current/lib/python3.10/site-packages/keras/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m np_utils\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelCheckpoint\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'np_utils' from 'keras.utils' (/home/codespace/.python/current/lib/python3.10/site-packages/keras/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "#necessary imports\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import glob,os\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "# from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24703be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from each .wav file\n",
    "def extract_features(data, sample_rate):\n",
    "    # ZCR\n",
    "    result = np.array([])\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result=np.hstack((result, zcr)) # stacking horizontally\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms)) # stacking horizontally\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel)) # stacking horizontally\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4429c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate = rate)\n",
    "\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sr = sampling_rate, n_steps = pitch_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f1828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions in the dataset\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  #'02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  #'07':'disgust',\n",
    "  #'08':'surprised'\n",
    "}\n",
    "\n",
    "#Emotions to observe\n",
    "#observed_emotions=['calm', 'happy', 'fearful', 'disgust', 'sad']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(path, isTraining):\n",
    "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
    "    #data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    data, sample_rate = librosa.load(path, duration=2, offset=0.6, sr=8025)\n",
    "    \n",
    "    # without augmentation\n",
    "    res1 = extract_features(data, sample_rate)\n",
    "    result = np.array(res1)\n",
    "    \n",
    "    # Augmenting only training data and skipping augmentation for test data\n",
    "    if isTraining:\n",
    "        \n",
    "        # data with noise\n",
    "        noise_data = noise(data)\n",
    "        res2 = extract_features(noise_data, sample_rate)\n",
    "        result = np.vstack((result, res2)) # stacking vertically\n",
    "\n",
    "        # data with stretching \n",
    "        stretched_data = stretch(data)\n",
    "        res3 = extract_features(stretched_data, sample_rate)\n",
    "        result = np.vstack((result, res3)) # stacking vertically\n",
    "        \n",
    "        # data with pitch offset\n",
    "        data_pitch = pitch(data, sample_rate)\n",
    "        res4 = extract_features(data_pitch, sample_rate)\n",
    "        result = np.vstack((result, res4)) # stacking vertically\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb984459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataframes\n",
    "Ravdess = \"data/\"\n",
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "print(ravdess_directory_list)\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for folder in glob.glob(f\"{Ravdess}Actor_*\"):\n",
    "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(folder)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        # third part in each file represents the emotion associated to that file.\n",
    "        if part[2] not in emotions.keys():\n",
    "            continue\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(folder + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "\n",
    "# dataframe for path of files\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Mapping integers to corresponding emotions\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear'}, inplace=True)\n",
    "Ravdess_df.head()\n",
    "\n",
    "X, Y = Ravdess_df[\"Path\"], Ravdess_df[\"Emotions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8323543",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for path, emotion in zip(X_train, Y_train):\n",
    "    feature = get_features(path, True)\n",
    "    for ele in feature:\n",
    "        X.append(ele)\n",
    "        # appending emotion 4 times as we have applied 4 augmentation techniques on each audio file\n",
    "        Y.append(emotion)\n",
    "x_train = X\n",
    "y_train = Y\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving features in a csv file to avoid redundant feature extraction\n",
    "Features = pd.DataFrame(x_train)\n",
    "Features['labels'] = y_train\n",
    "Features.to_csv('features.csv', index=False)\n",
    "Features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Features.iloc[: ,:-1].values\n",
    "y_train = Features['labels'].values\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cee47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = sklearn.preprocessing.OneHotEncoder()\n",
    "y_train = encoder.fit_transform(np.array(y_train).reshape(-1,1)).toarray()\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ad66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for path, emotion in zip(X_test, Y_test):\n",
    "    feature = get_features(path, False)   \n",
    "    X.append(feature)\n",
    "    Y.append(emotion)\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))\n",
    "x_test = np.array(X)\n",
    "y_test = np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb211d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = sklearn.preprocessing.OneHotEncoder()\n",
    "y_test = encoder.fit_transform(np.array(y_test).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76164e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=MLPClassifier(alpha=0.01, batch_size=64, epsilon=1e-08, hidden_layer_sizes=(500,), learning_rate='adaptive', max_iter=500)\n",
    "model1.fit(x_train,y_train)\n",
    "score = model1.score(x_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1164fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping train and test data for new model\n",
    "x_train = np.expand_dims(x_train, axis=2)\n",
    "x_test = np.expand_dims(x_test, axis=2)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7f26a",
   "metadata": {},
   "source": [
    "# old model\n",
    "model=Sequential()\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=8, activation='softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(256, 8, padding='same',activation='relu', input_shape=(x_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
    "model.add(Conv1D(256, 8, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 8, padding='same', activation='relu'))\n",
    "model.add(Conv1D(128, 8, padding='same',activation='relu'))\n",
    "model.add(Conv1D(128, 8, padding='same',activation='relu'))\n",
    "model.add(Conv1D(128, 8, padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(64, 8, padding='same',activation='relu'))\n",
    "model.add(Conv1D(64, 8, padding='same',activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(y_train.shape[1],activation='softmax')) # Target class number\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1a441",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
    "history=model.fit(x_train, y_train, batch_size=16, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of our model on test data : \" , model.evaluate(x_test, y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(x_test)\n",
    "y_pred = encoder.inverse_transform(pred_test)\n",
    "y_test = encoder.inverse_transform(y_test)\n",
    "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
    "df['Predicted Labels'] = y_pred.flatten()\n",
    "df['Actual Labels'] = y_test.flatten()\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
    "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d82401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model)\n",
    "tf.io.write_graph(onnx_model, '.', 'model.onnx', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66abf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved the current model to replicate results\n",
    "#dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b75741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e44fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(encoder, open('encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = onnx_model.graph.input[0].name\n",
    "output_name = onnx_model.graph.output[0].name\n",
    "\n",
    "print(\"Input node name:\", input_name)\n",
    "print(\"Output node name:\", output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b9b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
